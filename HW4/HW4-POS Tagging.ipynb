{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4 - Neural POS Tagger\n",
    "\n",
    "In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Tensorflow and Keras. Tensorflow is a deep learning framwork developed by Google, and Keras is a frontend library built on top of Tensorflow (or Theano, CNTK) to provide an easier way to use standard layers and networks.\n",
    "\n",
    "To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n",
    "\n",
    "- Neural POS Tagging with Word Embedding using Fixed / non-Fixed Pretrained weights\n",
    "- Neural POS Tagging with Viterbi / Marginal CRF\n",
    "\n",
    "Pretrained word embeddding are already given for you to use (albeit, a very bad one). Optionally, you can use your best pretrained word embeddding from previous exercise.\n",
    "\n",
    "We also provide the code for data cleaning, preprocessing and some starter code for keras in this notebook but feel free to modify those parts to suit your needs. You can also complete this exercise using only Tensorflow (without using Keras). Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
    "\n",
    "### Don't forget to shut down your instance on Gcloud when you are not using it ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use POS data from [ORCHID corpus](https://www.nectec.or.th/corpus/index.php?league=pm), which is a POS corpus for Thai language.\n",
    "A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n",
    "We also create a word vector for unknown word by random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data.orchid_corpus import get_sentences\n",
    "import numpy as np\n",
    "import numpy.random\n",
    "import keras.preprocessing\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('การ', 'FIXN'), ('ประชุม', 'VACT'), ('ทาง', 'NCMN'), ('วิชาการ', 'NCMN'), ('<space>', 'PUNC'), ('ครั้ง', 'CFQC'), ('ที่ 1', 'DONM')]\n"
     ]
    }
   ],
   "source": [
    "unk_emb =np.random.randn(32)\n",
    "train_data = get_sentences('train')\n",
    "test_data = get_sentences('test')\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = open('basic_ff_embedding.pt', 'rb')\n",
    "embeddings = pickle.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here [https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_idx ={}\n",
    "idx_to_word ={}\n",
    "label_to_idx = {}\n",
    "for sentence in train_data:\n",
    "    for word,pos in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)+1\n",
    "            idx_to_word[word_to_idx[word]] = word\n",
    "        if pos not in label_to_idx:\n",
    "            label_to_idx[pos] = len(label_to_idx)+1\n",
    "word_to_idx['UNK'] = len(word_to_idx)\n",
    "\n",
    "n_classes = len(label_to_idx.keys())+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2features(sent, i, emb):\n",
    "    word = sent[i][0]\n",
    "    if word in word_to_idx :\n",
    "        return word_to_idx[word]\n",
    "    else :\n",
    "        return word_to_idx['UNK']\n",
    "\n",
    "def sent2features(sent, emb_dict):\n",
    "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [word for (word, label) in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 29, 327,   5, 328])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_data[100], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create train and test dataset, then we use keras to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 355 ms, sys: 3.69 ms, total: 358 ms\n",
      "Wall time: 358 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
    "y_train = [sent2labels(sent) for sent in train_data]\n",
    "x_test = [sent2features(sent, embeddings) for sent in test_data]\n",
    "y_test = [sent2labels(sent) for sent in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train=keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_train=keras.preprocessing.sequence.pad_sequences(y_train, maxlen=None, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "x_test=keras.preprocessing.sequence.pad_sequences(x_test, maxlen=102, dtype='int32', padding='post', truncating='pre', value=0.)\n",
    "y_temp =[]\n",
    "for i in range(len(y_train)):\n",
    "    y_temp.append(np.eye(n_classes)[y_train[i]][np.newaxis,:])\n",
    "y_train = np.asarray(y_temp).reshape(-1,102,n_classes)\n",
    "del(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 29 327   5 328   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0] (18500, 102)\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.] (18500, 102, 48)\n"
     ]
    }
   ],
   "source": [
    "print(x_train[100],x_train.shape)\n",
    "print(y_train[100][3],y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output from keras is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n",
    "\n",
    "evaluation_report is the same as in the demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outputToLabel(yt,seq_len):\n",
    "    out = []\n",
    "    for i in range(0,len(yt)):\n",
    "        if(i==seq_len):\n",
    "            break\n",
    "        out.append(np.argmax(yt[i]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def evaluation_report(y_true, y_pred):\n",
    "    # retrieve all tags in y_true\n",
    "    tag_set = set()\n",
    "    for sent in y_true:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    for sent in y_pred:\n",
    "        for tag in sent:\n",
    "            tag_set.add(tag)\n",
    "    tag_list = sorted(list(tag_set))\n",
    "    \n",
    "    # count correct points\n",
    "    tag_info = dict()\n",
    "    for tag in tag_list:\n",
    "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
    "\n",
    "    all_correct = 0\n",
    "    all_count = sum([len(sent) for sent in y_true])\n",
    "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
    "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
    "            if tag_true == tag_pred:\n",
    "                tag_info[tag_true]['correct_tagged'] += 1\n",
    "                all_correct += 1\n",
    "            tag_info[tag_true]['y_true'] += 1\n",
    "            tag_info[tag_pred]['y_pred'] += 1\n",
    "    accuracy = (all_correct / all_count) * 100\n",
    "            \n",
    "    # summarize and make evaluation result\n",
    "    eval_list = list()\n",
    "    for tag in tag_list:\n",
    "        eval_result = dict()\n",
    "        eval_result['tag'] = tag\n",
    "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
    "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
    "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
    "        eval_result['precision'] = precision\n",
    "        eval_result['recall'] = recall\n",
    "        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
    "        \n",
    "        eval_list.append(eval_result)\n",
    "\n",
    "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(eval_list)\n",
    "    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Dense,GRU,Reshape,TimeDistributed,Bidirectional,Dropout,Masking\n",
    "from keras_contrib.layers import CRF\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is this section is separated to two groups\n",
    "\n",
    "- Neural POS Tagger (4.1)\n",
    "- Neural CRF POS Tagger (4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.1 Neural POS Tagger  (Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 496,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 131s - loss: 1.8997 - categorical_accuracy: 0.5464   \n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 119s - loss: 0.4277 - categorical_accuracy: 0.9024   \n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 119s - loss: 0.2553 - categorical_accuracy: 0.9351   \n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 119s - loss: 0.1997 - categorical_accuracy: 0.9461   \n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 119s - loss: 0.1721 - categorical_accuracy: 0.9521   \n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 119s - loss: 0.1563 - categorical_accuracy: 0.9552   \n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 120s - loss: 0.1445 - categorical_accuracy: 0.9579   \n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 119s - loss: 0.1361 - categorical_accuracy: 0.9600   \n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 120s - loss: 0.1289 - categorical_accuracy: 0.9617   \n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 120s - loss: 0.1229 - categorical_accuracy: 0.9630   \n",
      "CPU times: user 55min 5s, sys: 8min 56s, total: 1h 4min 1s\n",
      "Wall time: 20min 12s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe82116f048>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8092</td>\n",
       "      <td>99.3758</td>\n",
       "      <td>99.5921</td>\n",
       "      <td>3662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.8278</td>\n",
       "      <td>94.4714</td>\n",
       "      <td>94.6493</td>\n",
       "      <td>7792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.062</td>\n",
       "      <td>96.5184</td>\n",
       "      <td>93.7108</td>\n",
       "      <td>16301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9689</td>\n",
       "      <td>99.3654</td>\n",
       "      <td>99.6662</td>\n",
       "      <td>12840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>91.6667</td>\n",
       "      <td>98.5075</td>\n",
       "      <td>94.964</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.7817</td>\n",
       "      <td>87.5479</td>\n",
       "      <td>93.2653</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.6374</td>\n",
       "      <td>97.4026</td>\n",
       "      <td>97.5199</td>\n",
       "      <td>2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>67.3716</td>\n",
       "      <td>53.7349</td>\n",
       "      <td>59.7855</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>56.3725</td>\n",
       "      <td>62.5</td>\n",
       "      <td>59.2784</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>62.6316</td>\n",
       "      <td>42.5507</td>\n",
       "      <td>50.6742</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>98.8372</td>\n",
       "      <td>90.4255</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.4504</td>\n",
       "      <td>98.377</td>\n",
       "      <td>97.4042</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>89.3474</td>\n",
       "      <td>84.9471</td>\n",
       "      <td>87.0917</td>\n",
       "      <td>3053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>93.8809</td>\n",
       "      <td>94.7531</td>\n",
       "      <td>94.315</td>\n",
       "      <td>5201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>81.1055</td>\n",
       "      <td>71.9251</td>\n",
       "      <td>76.24</td>\n",
       "      <td>807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.5198</td>\n",
       "      <td>87.6353</td>\n",
       "      <td>88.0753</td>\n",
       "      <td>2105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>96.9589</td>\n",
       "      <td>92.8082</td>\n",
       "      <td>94.8381</td>\n",
       "      <td>542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>97.617</td>\n",
       "      <td>99.3074</td>\n",
       "      <td>98.4549</td>\n",
       "      <td>1147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.6471</td>\n",
       "      <td>96.2319</td>\n",
       "      <td>96.9343</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.6207</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.7778</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>94.9221</td>\n",
       "      <td>92.5413</td>\n",
       "      <td>93.7166</td>\n",
       "      <td>1402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>84.9498</td>\n",
       "      <td>79.2265</td>\n",
       "      <td>81.9884</td>\n",
       "      <td>1270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>89.9261</td>\n",
       "      <td>94.8972</td>\n",
       "      <td>92.3448</td>\n",
       "      <td>1339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>91.2114</td>\n",
       "      <td>83.9344</td>\n",
       "      <td>87.4217</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>92.3567</td>\n",
       "      <td>70.2179</td>\n",
       "      <td>79.7799</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>96.8944</td>\n",
       "      <td>88.6364</td>\n",
       "      <td>92.5816</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>93.1034</td>\n",
       "      <td>82.4427</td>\n",
       "      <td>87.4494</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>94.7531</td>\n",
       "      <td>97.1519</td>\n",
       "      <td>95.9375</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>70.7965</td>\n",
       "      <td>78.4314</td>\n",
       "      <td>74.4186</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>69.0265</td>\n",
       "      <td>75.7282</td>\n",
       "      <td>72.2222</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>65.2941</td>\n",
       "      <td>62.3596</td>\n",
       "      <td>63.7931</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>87.5</td>\n",
       "      <td>51.4706</td>\n",
       "      <td>64.8148</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>86.9712</td>\n",
       "      <td>91.4591</td>\n",
       "      <td>89.1587</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>62.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>75</td>\n",
       "      <td>85.7143</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>88.6957</td>\n",
       "      <td>100</td>\n",
       "      <td>94.0092</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>51.2821</td>\n",
       "      <td>57.971</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>75.6579</td>\n",
       "      <td>82.1429</td>\n",
       "      <td>78.7671</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>71.4286</td>\n",
       "      <td>75</td>\n",
       "      <td>73.1707</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>76.4706</td>\n",
       "      <td>86.6667</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=93.26</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8092  99.3758  99.5921          3662\n",
       "1                2   94.8278  94.4714  94.6493          7792\n",
       "2                3    91.062  96.5184  93.7108         16301\n",
       "3                4   99.9689  99.3654  99.6662         12840\n",
       "4                5   91.6667  98.5075   94.964            66\n",
       "5                6   99.7817  87.5479  93.2653           457\n",
       "6                7   97.6374  97.4026  97.5199          2025\n",
       "7                8   67.3716  53.7349  59.7855           223\n",
       "8                9   56.3725     62.5  59.2784           230\n",
       "9               10   62.6316  42.5507  50.6742           357\n",
       "10              11   83.3333  98.8372  90.4255            85\n",
       "11              12   96.4504   98.377  97.4042           788\n",
       "12              13   89.3474  84.9471  87.0917          3053\n",
       "13              14   93.8809  94.7531   94.315          5201\n",
       "14              15   81.1055  71.9251    76.24           807\n",
       "15              16   88.5198  87.6353  88.0753          2105\n",
       "16              17   96.9589  92.8082  94.8381           542\n",
       "17              18    97.617  99.3074  98.4549          1147\n",
       "18              19   97.6471  96.2319  96.9343           332\n",
       "19              20   98.6207  96.9492  97.7778           286\n",
       "20              21   94.9221  92.5413  93.7166          1402\n",
       "21              22   84.9498  79.2265  81.9884          1270\n",
       "22              23   89.9261  94.8972  92.3448          1339\n",
       "23              24   91.2114  83.9344  87.4217           768\n",
       "24              25   92.3567  70.2179  79.7799           290\n",
       "25              26   96.8944  88.6364  92.5816           156\n",
       "26              27   93.1034  82.4427  87.4494           108\n",
       "27              29   94.7531  97.1519  95.9375           307\n",
       "28              30   70.7965  78.4314  74.4186            80\n",
       "29              31   69.0265  75.7282  72.2222            78\n",
       "30              32   65.2941  62.3596  63.7931           111\n",
       "31              33      87.5  51.4706  64.8148            35\n",
       "32              34   86.9712  91.4591  89.1587           514\n",
       "33              35   71.4286  55.5556     62.5             5\n",
       "34              36       100       75  85.7143            12\n",
       "35              37   88.6957      100  94.0092           102\n",
       "36              38   66.6667  51.2821   57.971            20\n",
       "37              39   75.6579  82.1429  78.7671           115\n",
       "38              40       100      100      100           280\n",
       "39              41   71.4286       75  73.1707            15\n",
       "40              42       100  76.4706  86.6667            13\n",
       "41              43         0        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=93.26                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.4 s, sys: 7.66 s, total: 53 s\n",
      "Wall time: 18.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.save_weights('/data/exp_pos_no_crf.h5')\n",
    "#model.load_weights('/data/exp_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2 Neural POS Tagger - Fix Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 1\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. The word embedding should be fixed across training time. To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "(You may want to read about Keras's Masking layer)\n",
    "\n",
    "Optionally, you can use your own pretrained word embedding from previous homework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_em = []\n",
    "pre_em.append(np.zeros(32))\n",
    "for i in range(1,len(idx_to_word)+1):\n",
    "    if(idx_to_word[i] in embeddings.keys()):\n",
    "        pre_em.append(embeddings[idx_to_word[i]])\n",
    "    else:\n",
    "        pre_em.append(np.zeros(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15018"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 15,600\n",
      "Non-trainable params: 480,608\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True, weights=[np.array(pre_em)], trainable=False))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 119s - loss: 2.4451 - categorical_accuracy: 0.3027   \n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 119s - loss: 2.0917 - categorical_accuracy: 0.3625   \n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 119s - loss: 1.9125 - categorical_accuracy: 0.4373   \n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 120s - loss: 1.7347 - categorical_accuracy: 0.5000   \n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 120s - loss: 1.6138 - categorical_accuracy: 0.5380   \n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 119s - loss: 1.5232 - categorical_accuracy: 0.5659   \n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 120s - loss: 1.4557 - categorical_accuracy: 0.5867   \n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 119s - loss: 1.4041 - categorical_accuracy: 0.6005   \n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 119s - loss: 1.3650 - categorical_accuracy: 0.6123   \n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 119s - loss: 1.3301 - categorical_accuracy: 0.6238   \n",
      "CPU times: user 55min 8s, sys: 8min 50s, total: 1h 3min 58s\n",
      "Wall time: 19min 59s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8837a8a208>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>94.7097</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>97.0899</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>64.1585</td>\n",
       "      <td>65.9554</td>\n",
       "      <td>65.0445</td>\n",
       "      <td>5440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54.5816</td>\n",
       "      <td>64.2963</td>\n",
       "      <td>59.042</td>\n",
       "      <td>10859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>62.9213</td>\n",
       "      <td>85.2422</td>\n",
       "      <td>72.4004</td>\n",
       "      <td>11015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>22.2222</td>\n",
       "      <td>0.383142</td>\n",
       "      <td>0.753296</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>93.981</td>\n",
       "      <td>85.6181</td>\n",
       "      <td>89.6048</td>\n",
       "      <td>1780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>26.3158</td>\n",
       "      <td>4.81928</td>\n",
       "      <td>8.14664</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>16.1765</td>\n",
       "      <td>2.98913</td>\n",
       "      <td>5.04587</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "      <td>19.7674</td>\n",
       "      <td>33.0097</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>83.4094</td>\n",
       "      <td>68.4145</td>\n",
       "      <td>75.1715</td>\n",
       "      <td>548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>63.5338</td>\n",
       "      <td>42.3205</td>\n",
       "      <td>50.8016</td>\n",
       "      <td>1521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>67.548</td>\n",
       "      <td>78.1927</td>\n",
       "      <td>72.4816</td>\n",
       "      <td>4292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>30.9859</td>\n",
       "      <td>3.92157</td>\n",
       "      <td>6.96203</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>55.6725</td>\n",
       "      <td>50.6661</td>\n",
       "      <td>53.0514</td>\n",
       "      <td>1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>73.3032</td>\n",
       "      <td>27.7397</td>\n",
       "      <td>40.2484</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>75.9375</td>\n",
       "      <td>84.1558</td>\n",
       "      <td>79.8357</td>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>8.69565</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>92.4791</td>\n",
       "      <td>87.6568</td>\n",
       "      <td>90.0034</td>\n",
       "      <td>1328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>27.6119</td>\n",
       "      <td>2.30817</td>\n",
       "      <td>4.26022</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>44.9421</td>\n",
       "      <td>63.2884</td>\n",
       "      <td>52.5603</td>\n",
       "      <td>893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>63.1008</td>\n",
       "      <td>44.4809</td>\n",
       "      <td>52.1795</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>1.21065</td>\n",
       "      <td>2.36967</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>1.13636</td>\n",
       "      <td>2.23464</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>92.4528</td>\n",
       "      <td>15.5063</td>\n",
       "      <td>26.5583</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>83.501</td>\n",
       "      <td>73.8434</td>\n",
       "      <td>78.3758</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>90.5759</td>\n",
       "      <td>61.7857</td>\n",
       "      <td>73.4607</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=64.12</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision    recall   f_score correct_count\n",
       "0                1   94.7097   99.5929   97.0899          3670\n",
       "1                2   64.1585   65.9554   65.0445          5440\n",
       "2                3   54.5816   64.2963    59.042         10859\n",
       "3                4   62.9213   85.2422   72.4004         11015\n",
       "4                5         -         0         -             0\n",
       "5                6   22.2222  0.383142  0.753296             2\n",
       "6                7    93.981   85.6181   89.6048          1780\n",
       "7                8   26.3158   4.81928   8.14664            20\n",
       "8                9   16.1765   2.98913   5.04587            11\n",
       "9               10         -         0         -             0\n",
       "10              11       100   19.7674   33.0097            17\n",
       "11              12   83.4094   68.4145   75.1715           548\n",
       "12              13   63.5338   42.3205   50.8016          1521\n",
       "13              14    67.548   78.1927   72.4816          4292\n",
       "14              15   30.9859   3.92157   6.96203            44\n",
       "15              16   55.6725   50.6661   53.0514          1217\n",
       "16              17   73.3032   27.7397   40.2484           162\n",
       "17              18   75.9375   84.1558   79.8357           972\n",
       "18              19       100   8.69565        16            30\n",
       "19              20         -         0         -             0\n",
       "20              21   92.4791   87.6568   90.0034          1328\n",
       "21              22   27.6119   2.30817   4.26022            37\n",
       "22              23   44.9421   63.2884   52.5603           893\n",
       "23              24   63.1008   44.4809   52.1795           407\n",
       "24              25   55.5556   1.21065   2.36967             5\n",
       "25              26   66.6667   1.13636   2.23464             2\n",
       "26              27         -         0         -             0\n",
       "27              29   92.4528   15.5063   26.5583            49\n",
       "28              30         -         0         -             0\n",
       "29              31         -         0         -             0\n",
       "30              32         -         0         -             0\n",
       "31              33         -         0         -             0\n",
       "32              34    83.501   73.8434   78.3758           415\n",
       "33              35         -         0         -             0\n",
       "34              36         -         0         -             0\n",
       "35              37         -         0         -             0\n",
       "36              38         -         0         -             0\n",
       "37              39         -         0         -             0\n",
       "38              40   90.5759   61.7857   73.4607           173\n",
       "39              41         -         0         -             0\n",
       "40              42         -         0         -             0\n",
       "41              43         -         0         -             0\n",
       "42              45         -         0         -             0\n",
       "43              46         -         0         -             0\n",
       "44  accuracy=64.12                                            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.6 s, sys: 7.3 s, total: 52.9 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.save_weights('/data/fixw_pos_no_crf.h5')\n",
    "#model.load_weights('/data/exp_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 Neural POS Tagger - Trainable pretrained weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 2\n",
    "We would like you create a neural postagger model with keras with the pretrained word embedding as an input. However The word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus from previous homework.\n",
    "\n",
    "Optionally, you can use your own pretrained word embedding from previous homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "=================================================================\n",
      "Total params: 496,208\n",
      "Trainable params: 496,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# Write your code here\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True, weights=[np.array(pre_em)], trainable=True))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(n_classes,activation='softmax')))\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "18500/18500 [==============================] - 120s - loss: 1.9630 - categorical_accuracy: 0.5061   \n",
      "Epoch 2/10\n",
      "18500/18500 [==============================] - 120s - loss: 0.5473 - categorical_accuracy: 0.8760   \n",
      "Epoch 3/10\n",
      "18500/18500 [==============================] - 121s - loss: 0.3129 - categorical_accuracy: 0.9233   \n",
      "Epoch 4/10\n",
      "18500/18500 [==============================] - 120s - loss: 0.2347 - categorical_accuracy: 0.9399   \n",
      "Epoch 5/10\n",
      "18500/18500 [==============================] - 120s - loss: 0.1970 - categorical_accuracy: 0.9474   \n",
      "Epoch 6/10\n",
      "18500/18500 [==============================] - 120s - loss: 0.1745 - categorical_accuracy: 0.9523   \n",
      "Epoch 7/10\n",
      "18500/18500 [==============================] - 121s - loss: 0.1603 - categorical_accuracy: 0.9551   \n",
      "Epoch 8/10\n",
      "18500/18500 [==============================] - 121s - loss: 0.1493 - categorical_accuracy: 0.9576   \n",
      "Epoch 9/10\n",
      "18500/18500 [==============================] - 121s - loss: 0.1418 - categorical_accuracy: 0.9593   \n",
      "Epoch 10/10\n",
      "18500/18500 [==============================] - 121s - loss: 0.1340 - categorical_accuracy: 0.9606   \n",
      "CPU times: user 55min 25s, sys: 8min 56s, total: 1h 4min 21s\n",
      "Wall time: 20min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f88377034e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99.8911</td>\n",
       "      <td>99.5929</td>\n",
       "      <td>99.7418</td>\n",
       "      <td>3670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>94.9155</td>\n",
       "      <td>93.9258</td>\n",
       "      <td>94.418</td>\n",
       "      <td>7747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>91.199</td>\n",
       "      <td>96.6961</td>\n",
       "      <td>93.8671</td>\n",
       "      <td>16331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99.9379</td>\n",
       "      <td>99.5898</td>\n",
       "      <td>99.7636</td>\n",
       "      <td>12869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>84.4156</td>\n",
       "      <td>97.0149</td>\n",
       "      <td>90.2778</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>99.7817</td>\n",
       "      <td>87.5479</td>\n",
       "      <td>93.2653</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>97.824</td>\n",
       "      <td>97.3064</td>\n",
       "      <td>97.5645</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>69.209</td>\n",
       "      <td>59.0361</td>\n",
       "      <td>63.7191</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>70.3264</td>\n",
       "      <td>64.4022</td>\n",
       "      <td>67.234</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>63.4752</td>\n",
       "      <td>42.6698</td>\n",
       "      <td>51.0335</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>93.2584</td>\n",
       "      <td>96.5116</td>\n",
       "      <td>94.8571</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>96.8215</td>\n",
       "      <td>98.8764</td>\n",
       "      <td>97.8382</td>\n",
       "      <td>792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>88.7417</td>\n",
       "      <td>85.754</td>\n",
       "      <td>87.2223</td>\n",
       "      <td>3082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>95.268</td>\n",
       "      <td>93.8969</td>\n",
       "      <td>94.5775</td>\n",
       "      <td>5154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>81.268</td>\n",
       "      <td>75.4011</td>\n",
       "      <td>78.2247</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>88.4232</td>\n",
       "      <td>88.7177</td>\n",
       "      <td>88.5702</td>\n",
       "      <td>2131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>97.5758</td>\n",
       "      <td>82.7055</td>\n",
       "      <td>89.5273</td>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>98.1148</td>\n",
       "      <td>99.1342</td>\n",
       "      <td>98.6219</td>\n",
       "      <td>1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>97.3761</td>\n",
       "      <td>96.8116</td>\n",
       "      <td>97.093</td>\n",
       "      <td>334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>98.9619</td>\n",
       "      <td>96.9492</td>\n",
       "      <td>97.9452</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>94.0309</td>\n",
       "      <td>92.5413</td>\n",
       "      <td>93.2801</td>\n",
       "      <td>1402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>82.1086</td>\n",
       "      <td>80.1622</td>\n",
       "      <td>81.1237</td>\n",
       "      <td>1285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>89.1148</td>\n",
       "      <td>96.3147</td>\n",
       "      <td>92.5749</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>92.8659</td>\n",
       "      <td>82.5137</td>\n",
       "      <td>87.3843</td>\n",
       "      <td>755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>87.3199</td>\n",
       "      <td>73.3656</td>\n",
       "      <td>79.7368</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>94.7674</td>\n",
       "      <td>92.6136</td>\n",
       "      <td>93.6782</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>89.0625</td>\n",
       "      <td>87.0229</td>\n",
       "      <td>88.0309</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>95.0464</td>\n",
       "      <td>97.1519</td>\n",
       "      <td>96.0876</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>72</td>\n",
       "      <td>70.5882</td>\n",
       "      <td>71.2871</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>46.3158</td>\n",
       "      <td>85.4369</td>\n",
       "      <td>60.0683</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>79.2</td>\n",
       "      <td>55.618</td>\n",
       "      <td>65.3465</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>51.4706</td>\n",
       "      <td>63.6364</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>88.0623</td>\n",
       "      <td>90.5694</td>\n",
       "      <td>89.2982</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>83.3333</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>66.6667</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>100</td>\n",
       "      <td>87.5</td>\n",
       "      <td>93.3333</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>90.991</td>\n",
       "      <td>99.0196</td>\n",
       "      <td>94.8357</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>48.5714</td>\n",
       "      <td>43.5897</td>\n",
       "      <td>45.9459</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>81.2081</td>\n",
       "      <td>86.4286</td>\n",
       "      <td>83.737</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>100</td>\n",
       "      <td>64.7059</td>\n",
       "      <td>78.5714</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=93.37</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag precision   recall  f_score correct_count\n",
       "0                1   99.8911  99.5929  99.7418          3670\n",
       "1                2   94.9155  93.9258   94.418          7747\n",
       "2                3    91.199  96.6961  93.8671         16331\n",
       "3                4   99.9379  99.5898  99.7636         12869\n",
       "4                5   84.4156  97.0149  90.2778            65\n",
       "5                6   99.7817  87.5479  93.2653           457\n",
       "6                7    97.824  97.3064  97.5645          2023\n",
       "7                8    69.209  59.0361  63.7191           245\n",
       "8                9   70.3264  64.4022   67.234           237\n",
       "9               10   63.4752  42.6698  51.0335           358\n",
       "10              11   93.2584  96.5116  94.8571            83\n",
       "11              12   96.8215  98.8764  97.8382           792\n",
       "12              13   88.7417   85.754  87.2223          3082\n",
       "13              14    95.268  93.8969  94.5775          5154\n",
       "14              15    81.268  75.4011  78.2247           846\n",
       "15              16   88.4232  88.7177  88.5702          2131\n",
       "16              17   97.5758  82.7055  89.5273           483\n",
       "17              18   98.1148  99.1342  98.6219          1145\n",
       "18              19   97.3761  96.8116   97.093           334\n",
       "19              20   98.9619  96.9492  97.9452           286\n",
       "20              21   94.0309  92.5413  93.2801          1402\n",
       "21              22   82.1086  80.1622  81.1237          1285\n",
       "22              23   89.1148  96.3147  92.5749          1359\n",
       "23              24   92.8659  82.5137  87.3843           755\n",
       "24              25   87.3199  73.3656  79.7368           303\n",
       "25              26   94.7674  92.6136  93.6782           163\n",
       "26              27   89.0625  87.0229  88.0309           114\n",
       "27              29   95.0464  97.1519  96.0876           307\n",
       "28              30        72  70.5882  71.2871            72\n",
       "29              31   46.3158  85.4369  60.0683            88\n",
       "30              32      79.2   55.618  65.3465            99\n",
       "31              33   83.3333  51.4706  63.6364            35\n",
       "32              34   88.0623  90.5694  89.2982           509\n",
       "33              35   83.3333  55.5556  66.6667             5\n",
       "34              36       100     87.5  93.3333            14\n",
       "35              37    90.991  99.0196  94.8357           101\n",
       "36              38   48.5714  43.5897  45.9459            17\n",
       "37              39   81.2081  86.4286   83.737           121\n",
       "38              40       100      100      100           280\n",
       "39              41        75       75       75            15\n",
       "40              42       100  64.7059  78.5714            11\n",
       "41              43         -        0        -             0\n",
       "42              45         -        0        -             0\n",
       "43              46         -        0        -             0\n",
       "44  accuracy=93.37                                          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 s, sys: 7.4 s, total: 53.4 s\n",
      "Wall time: 18.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.save_weights('/data/nfixw_pos_no_crf.h5')\n",
    "#model.load_weights('/data/exp_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 3\n",
    "Compare the result between all neural tagger models in 4.1.x and provide a convincing reason and example for the result of these models (which model perform best or worst, why?)\n",
    "\n",
    "(If you use your own weight please state so in the answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1 CRF Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your next two tasks are to incorporate Conditional random fields (CRF) to your model. <b>You do not need to use pretrained weight</b>.\n",
    "\n",
    "Keras already implement a CRF neural model for you. However, you need to use the official extension repository for Keras library, call keras-contrib. You should read about keras-contrib crf layer before attempt this exercise section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 4\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>viterbi algorithm</b>. Your model must use crf for loss function and metric. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n",
    "\n",
    "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
    "\n",
    "Do not forget to save this model weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras_contrib.layers import CRF\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 32)           480608    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 102, 64)           12480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 102, 64)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 102, 48)           3120      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 102, 48)           0         \n",
      "_________________________________________________________________\n",
      "crf_1 (CRF)                  (None, 102, 48)           4752      \n",
      "=================================================================\n",
      "Total params: 500,960\n",
      "Trainable params: 500,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import keras.backend as K\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "# model.add(Masking(mask_value=0))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(n_classes)))\n",
    "model.add(Dropout(0.5))\n",
    "crf = CRF(n_classes,\n",
    "          learn_mode='join',\n",
    "          test_mode='viterbi',\n",
    "          sparse_target=True,\n",
    "          use_boundary=True,\n",
    "          use_bias=True,\n",
    "          activation='softmax',\n",
    "          kernel_initializer='glorot_uniform',\n",
    "          chain_initializer='orthogonal',\n",
    "          bias_initializer='zeros',\n",
    "          boundary_initializer='zeros',\n",
    "          kernel_regularizer=regularizers.l1_l2(0.),\n",
    "          chain_regularizer=regularizers.l1_l2(0.),\n",
    "          boundary_regularizer=regularizers.l1_l2(0.),\n",
    "          bias_regularizer=regularizers.l1_l2(0.),\n",
    "          kernel_constraint=None,\n",
    "          chain_constraint=None,\n",
    "          boundary_constraint=None,\n",
    "          bias_constraint=None,\n",
    "          input_dim=None,\n",
    "          unroll=False)\n",
    "model.add(crf)\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.01)\n",
    "model.compile(optimizer=adam,loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14800 samples, validate on 3700 samples\n",
      "Epoch 1/20\n",
      "14800/14800 [==============================] - 144s - loss: 2.7652 - acc: 0.2477 - val_loss: 1.6894 - val_acc: 0.8621\n",
      "Epoch 2/20\n",
      "14800/14800 [==============================] - 140s - loss: 0.8974 - acc: 0.8538 - val_loss: 0.4699 - val_acc: 0.8621\n",
      "Epoch 3/20\n",
      "14800/14800 [==============================] - 139s - loss: 0.4622 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 4/20\n",
      "14800/14800 [==============================] - 139s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4583 - val_acc: 0.8621\n",
      "Epoch 5/20\n",
      "14800/14800 [==============================] - 138s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4583 - val_acc: 0.8621\n",
      "Epoch 6/20\n",
      "14800/14800 [==============================] - 138s - loss: 0.4625 - acc: 0.8538 - val_loss: 0.4594 - val_acc: 0.8621\n",
      "Epoch 7/20\n",
      "14800/14800 [==============================] - 138s - loss: 0.4607 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 8/20\n",
      "14800/14800 [==============================] - 137s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 9/20\n",
      "14800/14800 [==============================] - 136s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 10/20\n",
      "14800/14800 [==============================] - 136s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 11/20\n",
      "14800/14800 [==============================] - 135s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 12/20\n",
      "14800/14800 [==============================] - 137s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 13/20\n",
      "14800/14800 [==============================] - 134s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 14/20\n",
      "14800/14800 [==============================] - 135s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 15/20\n",
      "14800/14800 [==============================] - 134s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 16/20\n",
      "14800/14800 [==============================] - 138s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4583 - val_acc: 0.8621\n",
      "Epoch 17/20\n",
      "14800/14800 [==============================] - 136s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 18/20\n",
      "14800/14800 [==============================] - 138s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 19/20\n",
      "14800/14800 [==============================] - 136s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "Epoch 20/20\n",
      "14800/14800 [==============================] - 137s - loss: 0.4597 - acc: 0.8538 - val_loss: 0.4582 - val_acc: 0.8621\n",
      "CPU times: user 2h 3min 58s, sys: 19min 48s, total: 2h 23min 47s\n",
      "Wall time: 45min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05,\n",
    "                              patience=3, min_lr=0.001)\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=20,verbose=1,shuffle=True,validation_split=0.2,\n",
    "         callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.05,\n",
    "                              patience=2, min_lr=0.0001)\n",
    "model.fit(x_train,y_train,batch_size=64,epochs=10,verbose=1,shuffle=True,validation_split=0.2,\n",
    "         callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "      <th>correct_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.26143</td>\n",
       "      <td>100</td>\n",
       "      <td>9.99688</td>\n",
       "      <td>3685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>45</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>46</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>accuracy=5.26</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag precision recall  f_score correct_count\n",
       "0               1   5.26143    100  9.99688          3685\n",
       "1               2         -      0        -             0\n",
       "2               3         -      0        -             0\n",
       "3               4         -      0        -             0\n",
       "4               5         -      0        -             0\n",
       "5               6         -      0        -             0\n",
       "6               7         -      0        -             0\n",
       "7               8         -      0        -             0\n",
       "8               9         -      0        -             0\n",
       "9              10         -      0        -             0\n",
       "10             11         -      0        -             0\n",
       "11             12         -      0        -             0\n",
       "12             13         -      0        -             0\n",
       "13             14         -      0        -             0\n",
       "14             15         -      0        -             0\n",
       "15             16         -      0        -             0\n",
       "16             17         -      0        -             0\n",
       "17             18         -      0        -             0\n",
       "18             19         -      0        -             0\n",
       "19             20         -      0        -             0\n",
       "20             21         -      0        -             0\n",
       "21             22         -      0        -             0\n",
       "22             23         -      0        -             0\n",
       "23             24         -      0        -             0\n",
       "24             25         -      0        -             0\n",
       "25             26         -      0        -             0\n",
       "26             27         -      0        -             0\n",
       "27             29         -      0        -             0\n",
       "28             30         -      0        -             0\n",
       "29             31         -      0        -             0\n",
       "30             32         -      0        -             0\n",
       "31             33         -      0        -             0\n",
       "32             34         -      0        -             0\n",
       "33             35         -      0        -             0\n",
       "34             36         -      0        -             0\n",
       "35             37         -      0        -             0\n",
       "36             38         -      0        -             0\n",
       "37             39         -      0        -             0\n",
       "38             40         -      0        -             0\n",
       "39             41         -      0        -             0\n",
       "40             42         -      0        -             0\n",
       "41             43         -      0        -             0\n",
       "42             45         -      0        -             0\n",
       "43             46         -      0        -             0\n",
       "44  accuracy=5.26                                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 11.3 s, total: 1min 18s\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# model.save_weights('/data/crf_viberbi.h5')\n",
    "#model.load_weights('/data/exp_pos_no_crf.h5')\n",
    "y_pred=model.predict(x_test)\n",
    "ypred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))]\n",
    "evaluation_report(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 CRF Marginal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 5\n",
    "\n",
    "Use Keras-contrib CRF layer in your model. You should set the layer parameter so it can give the best performance on testing using <b>marginal problabilities</b>. You <b>must not train the model</b> from scratch but use the pretrained weight from previous CRF Viterbi model.\n",
    "\n",
    "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_to_idx),32,input_length=102,mask_zero=True))\n",
    "# model.add(Masking(mask_value=0.))\n",
    "model.add(Bidirectional(GRU(32, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(n_classes, activation='sigmoid')))\n",
    "crf = CRF(n_classes,\n",
    "          learn_mode='marginal',\n",
    "          test_mode='marginal',\n",
    "          sparse_target=True,\n",
    "          use_boundary=True,\n",
    "          use_bias=True,\n",
    "          activation='softmax',\n",
    "          kernel_initializer='glorot_uniform',\n",
    "          chain_initializer='orthogonal',\n",
    "          bias_initializer='zeros',\n",
    "          boundary_initializer='zeros',\n",
    "          kernel_regularizer=None,\n",
    "          chain_regularizer=None,\n",
    "          boundary_regularizer=None,\n",
    "          bias_regularizer=None,\n",
    "          kernel_constraint=None,\n",
    "          chain_constraint=None,\n",
    "          boundary_constraint=None,\n",
    "          bias_constraint=None,\n",
    "          input_dim=None,\n",
    "          unroll=False)\n",
    "model.add(crf)\n",
    "model.summary()\n",
    "adam  = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,  loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x_train,y_train,batch_size=512,epochs=20,verbose=1,shuffle=True,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #TODO 6\n",
    "\n",
    "Please pick the best example that can show the different between CRF that use viterbi and CRF that use marginal problabilities. Compare the result and provide a convincing reason. (which model perform better, why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Write your answer here :</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
